{"cells":[{"cell_type":"markdown","source":["##### 1. Installing Required Dependencies\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e0efd452-201f-4e1f-8ee4-fb0004e4482a"},{"cell_type":"code","source":["!pip install praw"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:00:28.8202254Z","session_start_time":"2025-05-12T01:00:28.8212134Z","execution_start_time":"2025-05-12T01:00:41.2528991Z","execution_finish_time":"2025-05-12T01:00:59.4012342Z","parent_msg_id":"59050932-00c2-4934-9177-0c17fecf8541"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting praw\n  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting prawcore<3,>=2.4 (from praw)\n  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\nCollecting update_checker>=0.18 (from praw)\n  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: websocket-client>=0.54.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from praw) (0.58.0)\nRequirement already satisfied: requests<3.0,>=2.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from prawcore<3,>=2.4->praw) (2.31.0)\nRequirement already satisfied: six in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.2.2)\nDownloading praw-7.8.1-py3-none-any.whl (189 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\nDownloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\nInstalling collected packages: update_checker, prawcore, praw\nSuccessfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9724b5af-24b3-4cba-b425-9e49788086fb"},{"cell_type":"markdown","source":["##### 2. Importing Required Libraries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08ffecc2-bb3b-40b4-980f-4353975be812"},{"cell_type":"code","source":["import praw\n","import pandas as pd\n","from datetime import datetime\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, BooleanType"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:01:01.32128Z","session_start_time":null,"execution_start_time":"2025-05-12T01:01:01.3223429Z","execution_finish_time":"2025-05-12T01:01:03.6934206Z","parent_msg_id":"8e0740bb-5d83-4679-b9db-4bf703addcde"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1719e3a-017e-4a41-9d72-7ed8d0936add"},{"cell_type":"markdown","source":["##### 3. Initialize Spark and Spark session"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bfdf6437-410b-4770-9f4e-2e850208dacb"},{"cell_type":"code","source":["def init_fabric_spark():\n","    spark = SparkSession.builder \\\n","        .appName(\"Reddit Data Pipeline\") \\\n","        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n","        .config(\"spark.hadoop.fs.defaultFS\", \"abfss://APIDataLakehouse@onelake.dfs.fabric.microsoft.com\") \\\n","        .config(\"spark.driver.memory\", \"8g\") \\\n","        .config(\"spark.executor.memory\", \"4g\") \\\n","        .config(\"spark.executor.cores\", \"2\") \\\n","        .config(\"spark.num.executors\", \"2\") \\\n","        .getOrCreate()\n","\n","    return spark"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:01:05.7349312Z","session_start_time":null,"execution_start_time":"2025-05-12T01:01:05.7361047Z","execution_finish_time":"2025-05-12T01:01:06.0834244Z","parent_msg_id":"b09a79ea-adf9-4b09-9f22-a05836c8a433"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c764be2-efe2-48ac-835b-3b2a167083a3"},{"cell_type":"markdown","source":["\n","##### 4. Data Ingestion - Bronze Layer"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52450856-abd6-462f-a441-88c4bc4caae3"},{"cell_type":"code","source":["def define_schema():\n","    \"\"\"\n","    Defines the schema for the Spark DataFrame.\n","    \"\"\"\n","    return StructType([\n","        StructField(\"Submission_Fct_id\", StringType(), True), \n","        StructField(\"title\", StringType(), True),\n","        StructField(\"created_utc\", FloatType(), True), \n","        StructField(\"author_name\", StringType(), True),\n","        StructField(\"score\", IntegerType(), True), \n","        StructField(\"upvote_ratio\", FloatType(), True),\n","        StructField(\"num_comments\", IntegerType(), True), \n","        StructField(\"over_18\", BooleanType(), True),\n","        StructField(\"spoiler\", BooleanType(), True), \n","        StructField(\"link_flair_text\", StringType(), True),\n","        StructField(\"stickied\", BooleanType(), True), \n","        StructField(\"Total_Awards_Received\", IntegerType(), True),\n","        StructField(\"Gilded_Count\", IntegerType(), True), \n","        StructField(\"Number_of_Crossposts\", IntegerType(), True),\n","        StructField(\"url\", StringType(), True)\n","    ])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:01:07.6776658Z","session_start_time":null,"execution_start_time":"2025-05-12T01:01:07.6787123Z","execution_finish_time":"2025-05-12T01:01:08.0201827Z","parent_msg_id":"ad4ce24a-3bbc-4ac5-9e6d-db8cee8d4806"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e475da38-d511-4361-8b9f-2036f954fff7"},{"cell_type":"code","source":["def extract_reddit_data(client_id, client_secret, user_agent, subreddits, limit=100000):\n","    \"\"\"\n","    Extracts data from Reddit API and returns a pandas DataFrame.\n","    \"\"\"\n","    reddit = praw.Reddit(\n","        client_id=client_id, \n","        client_secret=client_secret, \n","        user_agent=user_agent, \n","        ratelimit_seconds=10\n","    )\n","\n","    # Combine data from different endpoints\n","    data = []\n","    \n","    # Get hot posts\n","    for submission in reddit.subreddit(subreddits).hot(limit=limit):\n","        data.append([\n","            submission.id, submission.title, submission.created_utc, \n","            str(submission.author), submission.score, submission.upvote_ratio,\n","            submission.num_comments, submission.over_18, submission.spoiler, \n","            submission.link_flair_text, submission.stickied,\n","            submission.total_awards_received, submission.gilded, \n","            submission.num_crossposts, submission.url\n","        ])\n","    \n","    # Get new posts\n","    for submission in reddit.subreddit(subreddits).new(limit=limit):\n","        data.append([\n","            submission.id, submission.title, submission.created_utc, \n","            str(submission.author), submission.score, submission.upvote_ratio,\n","            submission.num_comments, submission.over_18, submission.spoiler, \n","            submission.link_flair_text, submission.stickied,\n","            submission.total_awards_received, submission.gilded, \n","            submission.num_crossposts, submission.url\n","        ])\n","    \n","    # Get controversial posts\n","    for submission in reddit.subreddit(subreddits).controversial(limit=limit, time_filter=\"all\"):\n","        data.append([\n","            submission.id, submission.title, submission.created_utc, \n","            str(submission.author), submission.score, submission.upvote_ratio,\n","            submission.num_comments, submission.over_18, submission.spoiler, \n","            submission.link_flair_text, submission.stickied,\n","            submission.total_awards_received, submission.gilded, \n","            submission.num_crossposts, submission.url\n","        ])\n","    \n","    # Remove duplicates based on submission ID\n","    data = list({item[0]: item for item in data}.values())\n","    \n","    columns = [\"Submission_Fct_id\", \"title\", \"created_utc\", \"author_name\", \"score\", \"upvote_ratio\", \"num_comments\",\n","               \"over_18\", \"spoiler\", \"link_flair_text\", \"stickied\", \"Total_Awards_Received\",\n","               \"Gilded_Count\", \"Number_of_Crossposts\", \"url\"]\n","    \n","    # Define data types and create DataFrame\n","    return pd.DataFrame(data, columns=columns).astype({\n","        \"Submission_Fct_id\": \"string\", \"title\": \"string\", \"created_utc\": \"float\", \"author_name\": \"string\",\n","        \"score\": \"int\", \"upvote_ratio\": \"float\", \"num_comments\": \"int\", \"over_18\": \"bool\",\n","        \"spoiler\": \"bool\", \"link_flair_text\": \"string\", \"stickied\": \"bool\",\n","        \"Total_Awards_Received\": \"int\", \"Gilded_Count\": \"int\", \"Number_of_Crossposts\": \"int\", \"url\": \"string\"\n","    })"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:01:09.403597Z","session_start_time":null,"execution_start_time":"2025-05-12T01:01:09.4047312Z","execution_finish_time":"2025-05-12T01:01:09.7274571Z","parent_msg_id":"3be7fc1d-116f-49c4-8611-6f4b5784f6a0"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c610928d-5242-4c5e-beed-51b2389d0f3b"},{"cell_type":"markdown","source":["##### 5. Ingest into Data to Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"12ae4709-1e77-4eac-be45-bf6af92044f4"},{"cell_type":"code","source":["def upload_to_lakehouse(pdf, directory_path):\n","    \"\"\"\n","    Save pandas DataFrame as CSV file to Fabric Lakehouse\n","    \n","    Args:\n","        pdf: pandas DataFrame to save\n","        directory_path: Destination path within the Files folder (e.g., \"reddit_data/reddit_extract\")\n","    \"\"\"\n","    try:\n","        # Convert pandas DataFrame to Spark DataFrame\n","        spark_df = spark.createDataFrame(pdf)\n","        \n","        # ===== LAKEHOUSE CONFIGURATION =====\n","        # From your Properties screenshot:\n","        lakehouse_id = \"e466565d-d48f-47c5-a36c-129e9706433f\"  # Your Lakehouse ID\n","        workspace_name = \"APIDataProcessing-DP\"  # From your error message\n","        \n","        # ===== PATH CONSTRUCTION =====\n","        # Option 1: ABFS Path (most reliable)\n","\n","        full_path = f\"abfss://e466565d-d48f-47c5-a36c-129e9706433f@onelake.dfs.fabric.microsoft.com/05029953-93c2-40a0-b222-5656dc677253/Files/{directory_path}\"\n","        \n","        # Option 2: Simplified path (works if Lakehouse is attached)\n","        # full_path = f\"Files/{directory_path}\"\n","        \n","        # ===== DIRECTORY CREATION =====\n","        parent_dir = \"/\".join(directory_path.split(\"/\")[:-1])\n","        if parent_dir:\n","            mssparkutils.fs.mkdirs(f\"Files/{parent_dir}\")\n","        \n","        # ===== SAVE OPERATION =====\n","        spark_df.write \\\n","            .format(\"csv\") \\\n","            .option(\"header\", \"true\") \\\n","            .mode(\"overwrite\") \\\n","            .save(full_path)\n","        \n","        # ===== VERIFY OUTPUT =====\n","        csv_files = mssparkutils.fs.ls(f\"Files/{directory_path}\")\n","        csv_file = next(f for f in csv_files if f.name.startswith(\"part-\"))\n","        \n","        print(f\"‚úÖ CSV successfully saved to: {full_path}\")\n","        print(f\"üìÑ Actual file: {csv_file.path}\")\n","        return csv_file.path\n","        \n","    except Exception as e:\n","        print(f\"‚ùå Error saving to Lakehouse: {str(e)}\")\n","        print(\"Troubleshooting Tips:\")\n","        print(\"1. Verify Lakehouse is attached to notebook (top-right dropdown)\")\n","        print(\"2. Check if directory_path exists: mssparkutils.fs.ls('Files/')\")\n","        print(f\"3. Test manual save: df.write.csv('Files/test_output')\")\n","        raise\n","\n","def Load_to_delta_table_metastore(spark_df, database_name, table_name):\n","    \"\"\"\n","    Load data to a Delta table in the metastore, creating it if it doesn't exist\n","    \"\"\"\n","    # Create database if it doesn't exist\n","    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n","    \n","    full_table_name = f\"{database_name}.{table_name}\"\n","    \n","    if spark.catalog.tableExists(full_table_name):\n","        print(f\"Loading to existing table: {full_table_name}\")\n","        spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n","    else:\n","        print(f\"Creating new table: {full_table_name}\")\n","        spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)    \n","    print(f\"Data successfully written to {full_table_name}\")\n","    return full_table_name"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:01:11.6170132Z","session_start_time":null,"execution_start_time":"2025-05-12T01:01:11.6183259Z","execution_finish_time":"2025-05-12T01:01:11.9668765Z","parent_msg_id":"d12b4298-e436-441a-a088-74a14fffddc8"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0e06cd81-fc4f-4f40-a476-049d6023b92c"},{"cell_type":"markdown","source":["##### 5. Defining a helper function to run an Audit Trail"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32067a0d-1f95-46b8-b854-1cce95580c06"},{"cell_type":"code","source":["def log_audit_entry(action, status, details):\n","    \"\"\"\n","    Logs an audit entry into a Delta table.\n","    \"\"\"\n","    try:\n","        # Create a DataFrame for the audit log entry\n","        audit_data = [(action, status, details, datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"))]\n","        audit_schema = StructType([\n","            StructField(\"action\", StringType(), True),\n","            StructField(\"status\", StringType(), True),\n","            StructField(\"details\", StringType(), True),\n","            StructField(\"timestamp\", StringType(), True)\n","        ])\n","        \n","        audit_entry = spark.createDataFrame(audit_data, schema=audit_schema)\n","        \n","        # Database and table details\n","        database_name = \"audit_layer\"\n","        table_name = \"etl_logs\"\n","        full_table_name = f\"{database_name}.{table_name}\"\n","        \n","        # Ensure the database exists\n","        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n","        \n","        # Save the audit log\n","        if spark.catalog.tableExists(full_table_name):\n","            audit_entry.write.format(\"delta\").mode(\"append\").saveAsTable(full_table_name)\n","        else:\n","            audit_entry.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n","        \n","        print(f\"Audit log entry added successfully to {full_table_name}.\")\n","        return True\n","\n","    except Exception as e:\n","        print(f\"Failed to log audit entry: {e}\")\n","        raise"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:01:13.5192215Z","session_start_time":null,"execution_start_time":"2025-05-12T01:01:13.5204237Z","execution_finish_time":"2025-05-12T01:01:13.8713464Z","parent_msg_id":"2390785c-f83b-46b3-bfea-f2a33001b16e"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51e40eef-a932-40ad-b6df-1be3c332cc73"},{"cell_type":"markdown","source":["##### 7. Execute Complete Bronze Layer "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"863585c1-e1a3-4793-a1fa-c5ac358584aa"},{"cell_type":"markdown","source":["###### **Extraction:**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a35273f9-33cb-4d51-8439-d522d5257d9f"},{"cell_type":"code","source":["try:\n","    \n","    # Extraction parameters\n","    client_id = \"ABC\"\n","    client_secret = \"DEF\"\n","    user_agent = \"FahadHassan\"\n","    subreddits = \"funny+AskReddit+gaming+worldnews+todayilearned+aww+Music+memes+science+pics+Jokes+news+videos+space+askscience+DIY+books+food+mildlyinteresting+GetMotivated+explainlikeimfive+LifeProTips\"\n","        \n","    # Extract data from Reddit API\n","    print(\"Extracting data from Reddit API...\")\n","    pdf = extract_reddit_data(client_id, client_secret, user_agent, subreddits)\n","    print(f\"Successfully extracted {len(pdf)} records from Reddit\")\n","\n","    # Log extraction action\n","    log_audit_entry(\"Extract\", \"Success\", f\"Extracted {pdf.count()} records from Reddit API\")\n","\n","except Exception as e:\n","\n","        error_msg = f\"Error in Bronze layer processing: {str(e)}\"\n","        print(error_msg)\n","        log_audit_entry(\"BronzeError\", \"Failed\", error_msg)\n","        raise"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:01:17.2234521Z","session_start_time":null,"execution_start_time":"2025-05-12T01:01:17.2245096Z","execution_finish_time":"2025-05-12T01:11:13.1548967Z","parent_msg_id":"9717cc69-133a-4b08-a797-a926b661d3a4"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data from Reddit API...\nSuccessfully extracted 38482 records from Reddit\nAudit log entry added successfully to audit_layer.etl_logs.\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"462415c1-ab20-4136-be3a-fcbb65e4d1a4"},{"cell_type":"markdown","source":["###### **Ingesting in Lakehouse as .CSV**:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8b06aa4f-a2ba-410a-b50f-aae1a5944d79"},{"cell_type":"code","source":["try:\n","\n","    # Upload data to Lakehouse\n","    print(\"Uploading data to Lakehouse...\")\n","\n","    # Saving as .csv file in Lakehouse\n","    directory_path = \"reddit_data/reddit_extract\"\n","    lakehouse_path = f\"abfss://e466565d-d48f-47c5-a36c-129e9706433f@onelake.dfs.fabric.microsoft.com/05029953-93c2-40a0-b222-5656dc677253/Files/{directory_path}\"\n","    csv_file_path = upload_to_lakehouse(pdf, directory_path)\n","\n","    # Log upload action\n","    log_audit_entry(\"Upload\", \"Success\", f\"Uploaded data to {lakehouse_path}\")\n","\n","except Exception as e:\n","\n","    error_msg = f\"Error in Bronze layer processing: {str(e)}\"\n","    print(error_msg)\n","    log_audit_entry(\"BronzeError\", \"Failed\", error_msg)\n","    raise"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:12:57.5395069Z","session_start_time":null,"execution_start_time":"2025-05-12T01:12:57.5407616Z","execution_finish_time":"2025-05-12T01:13:02.4827677Z","parent_msg_id":"99fa3364-3a4a-4609-9e53-9671e544ca39"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Uploading data to Lakehouse...\n‚úÖ CSV successfully saved to: abfss://e466565d-d48f-47c5-a36c-129e9706433f@onelake.dfs.fabric.microsoft.com/05029953-93c2-40a0-b222-5656dc677253/Files/reddit_data/reddit_extract\nüìÑ Actual file: abfss://e466565d-d48f-47c5-a36c-129e9706433f@onelake.dfs.fabric.microsoft.com/05029953-93c2-40a0-b222-5656dc677253/Files/reddit_data/reddit_extract/part-00000-b454b279-0b86-4225-961b-19df95eed92f-c000.csv\nAudit log entry added successfully to audit_layer.etl_logs.\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59d982b9-6ed5-4dea-bd7f-27359eabdf14"},{"cell_type":"markdown","source":["###### **Ingesting Unique Data into Delta Lake:**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd6aca39-5ce0-4689-a3a8-0ebaf16bbd00"},{"cell_type":"code","source":["try:\n","    \n","    # Read data from Lakehouse into Spark DataFrame\n","    print(\"Reading data from Lakehouse...\")\n","    schema = define_schema()\n","\n","    spark_df = spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema) \\\n","        .option(\"quote\", '\\\"') \\\n","        .option(\"escape\", '\\\"') \\\n","        .option(\"multiline\", \"true\") \\\n","        .load(lakehouse_path)\n","\n","    # Drop duplicates based on the primary key column 'Submission_Fct_id'\n","    spark_df = spark_df.dropDuplicates([\"Submission_Fct_id\"])\n","        \n","    # Save to Bronze layer Delta table\n","    print(\"Saving to Bronze layer...\")\n","    bronze_table = Load_to_delta_table_metastore(spark_df, \"bronze_layer\", \"reddit_extracted_data\")\n","    \n","    # Log completion\n","    log_audit_entry(\"Bronze Load\", \"Success\", f\"Loaded data to {bronze_table}\")\n","    print(f\"Successfully completed Bronze layer processing. Data saved to {bronze_table}\")\n","    print(f\"Successfully loaded {spark_df.count()} records into Bronze layer\")\n","\n","except Exception as e:\n","    error_msg = f\"Error in Bronze layer processing: {str(e)}\"\n","    print(error_msg)\n","    log_audit_entry(\"BronzeError\", \"Failed\", error_msg)\n","    raise"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99","normalized_state":"finished","queued_time":"2025-05-12T01:13:16.9033476Z","session_start_time":null,"execution_start_time":"2025-05-12T01:13:16.9044898Z","execution_finish_time":"2025-05-12T01:13:27.2114923Z","parent_msg_id":"cfc4526c-fde5-43d2-b9a0-60916bd0f51a"},"text/plain":"StatementMeta(, 375d0ff8-2ad2-4dab-8511-d5b5b83d1e99, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Reading data from Lakehouse...\nSaving to Bronze layer...\nLoading to existing table: bronze_layer.reddit_extracted_data\nData successfully written to bronze_layer.reddit_extracted_data\nAudit log entry added successfully to audit_layer.etl_logs.\nSuccessfully completed Bronze layer processing. Data saved to bronze_layer.reddit_extracted_data\nSuccessfully loaded 38450 records into Bronze layer\n"]}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"bf46b4b5-bea0-4904-aa90-5660d15f915c\",\"activityId\":\"375d0ff8-2ad2-4dab-8511-d5b5b83d1e99\",\"applicationId\":\"application_1747011344068_0001\",\"jobGroupId\":\"14\",\"advices\":{\"info\":2}}"}},"id":"b0aa5665-f4d2-4a3a-8fdd-1ba519de1313"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"05029953-93c2-40a0-b222-5656dc677253"}],"default_lakehouse":"05029953-93c2-40a0-b222-5656dc677253","default_lakehouse_name":"APIDataLakehouse","default_lakehouse_workspace_id":"e466565d-d48f-47c5-a36c-129e9706433f"}}},"nbformat":4,"nbformat_minor":5}
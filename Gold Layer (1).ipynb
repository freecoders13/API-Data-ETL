{"cells":[{"cell_type":"markdown","source":["##### 1. Import Required Libraries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"91aa4fba-8622-4142-825c-ffadf196f77f"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import (\n","    StructType, \n","    StructField, \n","    LongType, \n","    StringType, \n","    BooleanType, \n","    TimestampType, \n","    IntegerType, \n","    FloatType, \n","    DoubleType\n",")\n","from pyspark.sql.functions import (\n","    lit, \n","    current_timestamp, \n","    col, \n","    when, \n","    row_number, \n","    lag, \n","    broadcast\n",")\n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"360bc22a-55d4-49d9-99af-7aa4b0b05708","normalized_state":"finished","queued_time":"2025-05-18T21:25:09.6681439Z","session_start_time":null,"execution_start_time":"2025-05-18T21:25:09.6693609Z","execution_finish_time":"2025-05-18T21:25:13.4139627Z","parent_msg_id":"fda96d41-d503-46a4-adc5-fcd199412eb9"},"text/plain":"StatementMeta(, 360bc22a-55d4-49d9-99af-7aa4b0b05708, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31c1b07b-26ba-497f-b46b-da7950bd084f"},{"cell_type":"markdown","source":["##### 2. Creting Dimension Tables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6371c844-758b-4eef-82e8-4622ebab164a"},{"cell_type":"code","source":["def create_dim_author(sdf, existing_dim_table=None):\n","    \"\"\"\n","    Implements SCD Type 2 logic for the author dimension without using window functions.\n","    \n","    :param sdf: Source DataFrame containing new author records\n","    :param existing_dim_table: Full table name of existing dimension (e.g., \"mydb.dim_author\")\n","    :return: DataFrame with updated SCD2-compliant data\n","    \"\"\"\n","    now = current_timestamp()\n","\n","    # Prepare new incoming data\n","    new_data = sdf.select(\"pk_author_id\", \"sk_author_id\", \"author_name\") \\\n","                  .withColumn(\"valid_from\", now) \\\n","                  .withColumn(\"valid_to\", lit('9999-12-31 23:59:59.999').cast(\"timestamp\")) \\\n","                  .withColumn(\"is_current\", lit(True)) \\\n","                  .dropDuplicates([\"sk_author_id\"])\n","\n","    if not existing_dim_table or not spark.catalog.tableExists(existing_dim_table):\n","        return new_data\n","\n","    # Load existing dimension table\n","    existing_data = spark.table(existing_dim_table)\n","\n","    # Filter active records (is_current = True)\n","    current_records = existing_data.filter(\"is_current = True\")\n","\n","    # Identify records to expire (changed author_name)\n","    records_to_expire = current_records.alias(\"e\").join(\n","        new_data.alias(\"n\"), on=\"sk_author_id\"\n","    ).filter(\"e.author_name != n.author_name\") \\\n","     .select(\"e.*\") \\\n","     .withColumn(\"is_current\", lit(False)) \\\n","     .withColumn(\"valid_to\", now)\n","\n","    # Truly new records (sk_author_id not in current table)\n","    new_sk_ids = new_data.alias(\"n\").join(\n","        current_records.alias(\"e\"),\n","        on=\"sk_author_id\",\n","        how=\"left_anti\"\n","    )\n","\n","    # New versions of changed records\n","    changed_records = new_data.alias(\"n\").join(\n","        current_records.alias(\"e\"),\n","        on=\"sk_author_id\"\n","    ).filter(\"e.author_name != n.author_name\") \\\n","     .select(\"n.*\")\n","\n","    records_to_insert = new_sk_ids.unionByName(changed_records)\n","\n","    # Final union: existing unchanged + expired + inserts\n","    unchanged_records = existing_data.alias(\"e\").join(\n","        records_to_expire.alias(\"x\"), on=\"sk_author_id\", how=\"left_anti\"\n","    ).join(\n","        records_to_insert.alias(\"n\"), on=\"sk_author_id\", how=\"left_anti\"\n","    )\n","\n","    final_dim_df = unchanged_records.unionByName(records_to_expire).unionByName(records_to_insert)\n","\n","    return final_dim_df\n","\n","\n","\n","def create_dim_date(sdf):\n","\n","    # Select date-related columns\n","    new_data = sdf.select(\"pk_date_id\", \"sk_date_id\", \"year\", \"month\", \"day\", \"weekday\").dropDuplicates([\"sk_date_id\"])\n","\n","    return new_data\n","\n","def create_dim_time(sdf):\n","\n","    # Select time-related columns\n","    new_data = sdf.select(\"pk_time_id\", \"sk_time_id\", \"hour\").dropDuplicates([\"sk_time_id\"])\n","\n","    return new_data\n","\n","def create_dim_post(sdf):\n","\n","    # Select post-related columns\n","    new_data = sdf.select(\"pk_post_id\", \"sk_post_id\", \"title\", \"title_length\", \"link_flair_text\", \"url\").dropDuplicates([\"sk_post_id\"])\n","\n","    return new_data\n","\n","def create_dim_engagement_attributes(sdf):\n","\n","    # Select engagement-related columns\n","    new_data = sdf.select(\"pk_engagement_id\", \"sk_engagement_id\", \"is_adult_content\", \"is_spoiler\", \"is_stickied\", \"has_awards\", \"has_crossposts\").dropDuplicates([\"sk_engagement_id\"])\n","    \n","    return new_data    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"360bc22a-55d4-49d9-99af-7aa4b0b05708","normalized_state":"finished","queued_time":"2025-05-18T21:25:14.4695915Z","session_start_time":null,"execution_start_time":"2025-05-18T21:25:14.4709184Z","execution_finish_time":"2025-05-18T21:25:51.6518435Z","parent_msg_id":"d4516e69-30d0-480f-bb22-0edbb06e50ee"},"text/plain":"StatementMeta(, 360bc22a-55d4-49d9-99af-7aa4b0b05708, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8aeed21a-533d-4ec2-9331-3f83796c86dd"},{"cell_type":"markdown","source":["##### 3. Save dimension tables using merge\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7ffa2ad3-d462-4393-8cce-1872bed3fcf9"},{"cell_type":"code","source":["def save_dim_table(df, table_name, database_name, partition_columns=None, match_condition=None):\n","    \"\"\"\n","    Save DataFrame as a Delta table in Microsoft Fabric with upsert functionality using Spark SQL.\n","    \n","    :param df: DataFrame to save\n","    :param table_name: Table name\n","    :param database_name: Database name\n","    :param partition_columns: List of columns to partition by\n","    :param match_condition: SQL condition for matching records (e.g., \"target.id = source.id\")\n","    \"\"\"\n","    # Create the database if it doesn't exist\n","    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n","\n","    full_table_name = f\"{database_name}.{table_name}\"\n","\n","    # Check if the table exists\n","    table_exists = spark.catalog.tableExists(full_table_name)\n","\n","    if not table_exists:\n","\n","        # If table doesn't exist, create it\n","        writer = df.write.format(\"delta\")\n","        if partition_columns:\n","            writer = writer.partitionBy(*partition_columns)\n","        writer.mode(\"overwrite\").saveAsTable(full_table_name)\n","        print(f\"Table {full_table_name} created.\")\n","\n","    else:\n","\n","        # If table exists, perform upsert using MERGE INTO\n","        if not match_condition:\n","            raise ValueError(\"match_condition is required for upsert operations.\")\n","\n","        # Register the source DataFrame as a temp view\n","        df.createOrReplaceTempView(\"source_view\")\n","\n","        # Build and run the MERGE SQL\n","        merge_sql = f\"\"\"\n","        MERGE INTO {full_table_name} AS target\n","        USING source_view AS source\n","        ON {match_condition}\n","        WHEN MATCHED THEN\n","          UPDATE SET *\n","        WHEN NOT MATCHED THEN\n","          INSERT *\n","        \"\"\"\n","        spark.sql(merge_sql)\n","        \n","        print(f\"Upsert completed for table {full_table_name}.\")\n","\n","def save_SCD_2_dim_table(df, table_name, database_name, partition_columns=None, match_condition=None):\n","    \"\"\"\n","    Save DataFrame as a Delta table in Microsoft Fabric or Databricks with upsert (MERGE) logic.\n","    \n","    :param df: DataFrame to save\n","    :param table_name: Table name\n","    :param database_name: Database name\n","    :param partition_columns: List of columns to partition by\n","    :param match_condition: SQL condition for matching (e.g., \"target.sk_author_id = source.sk_author_id\")\n","    \"\"\"\n","    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n","    full_table_name = f\"{database_name}.{table_name}\"\n","    table_exists = spark.catalog.tableExists(full_table_name)\n","\n","    if not table_exists:\n","        writer = df.write.format(\"delta\")\n","        if partition_columns:\n","            writer = writer.partitionBy(*partition_columns)\n","        writer.mode(\"overwrite\").saveAsTable(full_table_name)\n","        print(f\"Table {full_table_name} created.\")\n","    else:\n","        if not match_condition:\n","            raise ValueError(\"match_condition is required for upsert operations.\")\n","\n","        # Register the source DataFrame\n","        df.createOrReplaceTempView(\"source_view\")\n","\n","        # Dynamically build column mappings\n","        columns = df.columns\n","        update_clause = \", \".join([f\"target.{col} = source.{col}\" for col in columns])\n","        insert_cols = \", \".join(columns)\n","        insert_vals = \", \".join([f\"source.{col}\" for col in columns])\n","\n","        merge_sql = f\"\"\"\n","        MERGE INTO {full_table_name} AS target\n","        USING source_view AS source\n","        ON {match_condition} AND target.is_current = TRUE\n","        WHEN MATCHED THEN\n","          UPDATE SET {update_clause}\n","        WHEN NOT MATCHED THEN\n","          INSERT ({insert_cols}) VALUES ({insert_vals})\n","        \"\"\"\n","        spark.sql(merge_sql)\n","        print(f\"Upsert completed for table {full_table_name}.\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"360bc22a-55d4-49d9-99af-7aa4b0b05708","normalized_state":"finished","queued_time":"2025-05-18T21:25:18.4509461Z","session_start_time":null,"execution_start_time":"2025-05-18T21:25:51.6541459Z","execution_finish_time":"2025-05-18T21:26:12.3165212Z","parent_msg_id":"f409afdd-6373-45f3-932c-b4a384969ec8"},"text/plain":"StatementMeta(, 360bc22a-55d4-49d9-99af-7aa4b0b05708, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52bac6dd-fa77-46b4-9e9c-d80f76b02461"},{"cell_type":"markdown","source":["##### 4. Create the fact table\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d1c7846-f457-404d-94d7-a709f7c26648"},{"cell_type":"code","source":["def create_fact_post_metrics(sdf, dim_author, dim_date, dim_time, dim_post, dim_engagement_attributes):\n","\n","    # Defining alias\n","    dim_author = dim_author.alias(\"da\")\n","    dim_date = dim_date.alias(\"dd\")\n","    dim_time = dim_time.alias(\"dt\")\n","    dim_post = dim_post.alias(\"dp\")\n","    dim_engagement_attributes = dim_engagement_attributes.alias(\"de\")\n","    new_data = sdf.alias(\"nd\")\n","\n","    # Perform joins\n","    fact_table = new_data \\\n","        .join(broadcast(dim_author), col(\"nd.sk_author_id\") == col(\"da.sk_author_id\"), \"left\") \\\n","        .join(broadcast(dim_date), col(\"nd.sk_date_id\") == col(\"dd.sk_date_id\"), \"left\") \\\n","        .join(broadcast(dim_time), col(\"nd.sk_time_id\") == col(\"dt.sk_time_id\"), \"left\") \\\n","        .join(broadcast(dim_post), col(\"nd.sk_post_id\") == col(\"dp.sk_post_id\"), \"left\") \\\n","        .join(broadcast(dim_engagement_attributes), col(\"nd.sk_engagement_id\") == col(\"de.sk_engagement_id\"), \"left\") \\\n","        .select(\n","            col(\"da.pk_author_id\"),\n","            col(\"dd.pk_date_id\"),\n","            col(\"dt.pk_time_id\"),\n","            col(\"dp.pk_post_id\"),\n","            col(\"de.pk_engagement_id\"),\n","            col(\"nd.Submission_Fct_id\"),\n","            col(\"nd.sk_author_id\"),\n","            col(\"nd.sk_date_id\"),\n","            col(\"nd.sk_time_id\"),\n","            col(\"nd.sk_post_id\"),\n","            col(\"nd.sk_engagement_id\"),\n","            col(\"nd.engagement_score\"),\n","            col(\"nd.award_rate\"),\n","            col(\"nd.score_upvote_ratio\"),\n","            col(\"nd.engagement_ratio\"),\n","            col(\"nd.Total_Awards_Received\"),\n","            col(\"nd.Gilded_Count\"),\n","            col(\"nd.Number_of_Crossposts\"),\n","            col(\"nd.score\"),\n","            col(\"nd.upvote_ratio\"),\n","            col(\"nd.num_comments\")\n","        )\n","\n","    return fact_table"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"360bc22a-55d4-49d9-99af-7aa4b0b05708","normalized_state":"finished","queued_time":"2025-05-18T21:26:15.8349857Z","session_start_time":null,"execution_start_time":"2025-05-18T21:26:15.8362183Z","execution_finish_time":"2025-05-18T21:26:43.4009077Z","parent_msg_id":"093c6ed6-6071-48b6-bb45-c27f280b26b1"},"text/plain":"StatementMeta(, 360bc22a-55d4-49d9-99af-7aa4b0b05708, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19b04922-22a1-41c9-ba65-2407642b3e0d"},{"cell_type":"markdown","source":["##### 5. Incrementally save fact table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e6c50d4-b4ba-431e-bb13-24e09b902760"},{"cell_type":"code","source":["def incremental_load_fact_table(new_data, table_name, database_name, partition_columns=None):\n","    \"\"\"\n","    SCD Type 2 incremental load with support for multiple source rows per key.\n","    \"\"\"\n","    full_table_name = f\"{database_name}.{table_name}\"\n","    table_exists = spark.catalog.tableExists(full_table_name)\n","\n","    # Add SCD2 columns to new data\n","    new_data_with_flags = new_data.withColumn(\"valid_from\", current_timestamp()) \\\n","                                  .withColumn(\"valid_to\", lit(\"9999-12-31 23:59:59.999\")) \\\n","                                  .withColumn(\"is_current\", lit(True))\n","\n","    if not table_exists:\n","        writer = new_data_with_flags.write.format(\"delta\").mode(\"overwrite\")\n","        if partition_columns:\n","            writer = writer.partitionBy(*partition_columns)\n","        writer.saveAsTable(full_table_name)\n","        print(f\"✅ Created table: {full_table_name}\")\n","        return\n","\n","    # Step 1: Deduplicate source keys for safe merge\n","    new_data_with_flags.select(\"Submission_Fct_id\").distinct().createOrReplaceTempView(\"deduped_source_keys\")\n","\n","\n","    # Step 2: Expire active target records matching deduped source keys\n","    spark.sql(f\"\"\"\n","    MERGE INTO {full_table_name} AS target\n","    USING deduped_source_keys AS source\n","    ON target.Submission_Fct_id = source.Submission_Fct_id\n","        AND target.valid_to = '9999-12-31 23:59:59.999'\n","        AND target.is_current = true\n","    WHEN MATCHED THEN UPDATE SET\n","        target.valid_to = current_timestamp(),\n","        target.is_current = false\n","    \"\"\")\n","\n","    # Step 3: Append all new source records (including duplicates)\n","    new_data_with_flags.write.format(\"delta\").mode(\"append\").saveAsTable(full_table_name)\n","    print(f\"✅ Incremental SCD2 load completed for table: {full_table_name}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"360bc22a-55d4-49d9-99af-7aa4b0b05708","normalized_state":"finished","queued_time":"2025-05-18T21:27:22.5270788Z","session_start_time":null,"execution_start_time":"2025-05-18T21:27:22.5282463Z","execution_finish_time":"2025-05-18T21:27:32.4944837Z","parent_msg_id":"ee66eb15-8797-4c14-9822-8add5e471294"},"text/plain":"StatementMeta(, 360bc22a-55d4-49d9-99af-7aa4b0b05708, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9695ba2f-acfb-4caf-b62d-8cf1889897bb"},{"cell_type":"markdown","source":["##### 6. Execute full pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d395c033-b423-4173-8995-a92b55d5a4f2"},{"cell_type":"code","source":["\n","\"\"\"\n","Select Silver_Layer.transformed_data to perform DWH Loading\n","\"\"\"\n","\n","sdf = spark.sql(\"SELECT * FROM Silver_Layer.transformed_data\")\n","\n","\"\"\"\n","Creates and saves all dimension and fact tables to a Hive Metastore database.\n","Overwrites dimension tables and appends to the fact table.\n","\"\"\"\n","\n","database_name = \"gold_dimensional_modeling\"\n","\n","# Create dimension tables\n","dim_author = create_dim_author(sdf, \"gold_dimensional_modeling.dim_author\")            \n","dim_date = create_dim_date(sdf)\n","dim_time = create_dim_time(sdf)\n","dim_post = create_dim_post(sdf)\n","dim_engagement_attributes = create_dim_engagement_attributes(sdf)\n","\n","# Save dimension tables (overwrite mode)\n","save_SCD_2_dim_table(dim_author, table_name=\"dim_author\", database_name=database_name, partition_columns=[\"valid_from\", \"valid_to\"], match_condition=\"target.sk_author_id = source.sk_author_id\")\n","save_dim_table(dim_date, \"dim_date\", database_name, partition_columns=[\"year\", \"month\"], match_condition=\"target.sk_date_id == source.sk_date_id\")\n","save_dim_table(dim_time, \"dim_time\", database_name, match_condition=\"target.sk_time_id == source.sk_time_id\")\n","save_dim_table(dim_post, \"dim_post\", database_name, match_condition=\"target.sk_post_id == source.sk_post_id\")\n","save_dim_table(dim_engagement_attributes, \"dim_engagement_attributes\", database_name, match_condition=\"target.sk_engagement_id == source.sk_engagement_id\")\n","\n","\n","# Create fact table\n","new_data = create_fact_post_metrics(sdf, dim_author, dim_date, dim_time, dim_post, dim_engagement_attributes)\n","\n","# Save fact table (with existence check and partitioning)\n","incremental_load_fact_table(new_data, \"fact_Submissions\", database_name=database_name, partition_columns= [\"valid_from\", \"valid_to\"])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"360bc22a-55d4-49d9-99af-7aa4b0b05708","normalized_state":"finished","queued_time":"2025-05-18T21:27:30.5368624Z","session_start_time":null,"execution_start_time":"2025-05-18T21:27:32.4966457Z","execution_finish_time":"2025-05-18T21:30:35.1668031Z","parent_msg_id":"8b94f5cc-fa63-47ae-b36d-ed77fb78b68e"},"text/plain":"StatementMeta(, 360bc22a-55d4-49d9-99af-7aa4b0b05708, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Upsert completed for table gold_dimensional_modeling.dim_author.\nUpsert completed for table gold_dimensional_modeling.dim_date.\nUpsert completed for table gold_dimensional_modeling.dim_time.\nUpsert completed for table gold_dimensional_modeling.dim_post.\nUpsert completed for table gold_dimensional_modeling.dim_engagement_attributes.\n✅ Incremental SCD2 load completed for table: gold_dimensional_modeling.fact_Submissions\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b497791f-748c-4190-97a3-edd33a3ec1f9"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"05029953-93c2-40a0-b222-5656dc677253"}],"default_lakehouse":"05029953-93c2-40a0-b222-5656dc677253","default_lakehouse_name":"APIDataLakehouse","default_lakehouse_workspace_id":"e466565d-d48f-47c5-a36c-129e9706433f"}}},"nbformat":4,"nbformat_minor":5}
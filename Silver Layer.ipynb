{"cells":[{"cell_type":"markdown","source":["##### 1. Import Required Libraries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4c25ce5e-4ec2-4432-9957-ebe29c642fc3"},{"cell_type":"code","source":["import pandas as pd\n","from datetime import datetime\n","from pyspark.sql import SparkSession, DataFrame\n","from pyspark.sql.window import Window\n","from pyspark.sql import functions as F\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, FloatType, DoubleType,\n","    LongType, IntegerType, BooleanType\n",")\n","from pyspark.sql.functions import (\n","    col, when, row_number, coalesce,\n","    from_unixtime, current_date, datediff,\n","    lit, current_timestamp, concat_ws,\n","    crc32, md5\n",")\n","from delta.tables import DeltaTable"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"dbdf5672-6fd7-410b-b31f-0e00edc71b66","normalized_state":"finished","queued_time":"2025-05-12T22:47:53.6040049Z","session_start_time":null,"execution_start_time":"2025-05-12T22:47:58.2154805Z","execution_finish_time":"2025-05-12T22:48:02.8129275Z","parent_msg_id":"8226c983-0ff4-423b-ae38-a540174b2547"},"text/plain":"StatementMeta(, dbdf5672-6fd7-410b-b31f-0e00edc71b66, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad8b58f6-48dd-4c1b-b08a-55ec1642087b"},{"cell_type":"markdown","source":["##### 2. Define Silver Layer Schema"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bf5dc533-eb57-48d7-afc5-1b77e4f77e5c"},{"cell_type":"code","source":["silver_layer_schema = StructType([\n","    StructField(\"Submission_Fct_id\", StringType(), False),\n","    StructField(\"pk_engagement_id\", LongType(), False),\n","    StructField(\"pk_post_id\", LongType(), False),\n","    StructField(\"pk_time_id\", LongType(), False),\n","    StructField(\"pk_date_id\", LongType(), False),\n","    StructField(\"pk_author_id\", LongType(), False),\n","    StructField(\"sk_engagement_id\", LongType(), False),\n","    StructField(\"sk_post_id\", LongType(), False),\n","    StructField(\"sk_time_id\", LongType(), False),\n","    StructField(\"sk_date_id\", LongType(), False),\n","    StructField(\"sk_author_id\", LongType(), False),\n","    StructField(\"author_name\", StringType(), True),\n","    StructField(\"year\", IntegerType(), True),\n","    StructField(\"month\", IntegerType(), True),\n","    StructField(\"day\", IntegerType(), True),\n","    StructField(\"hour\", IntegerType(), True),\n","    StructField(\"weekday\", StringType(), True),\n","    StructField(\"num_comments\", IntegerType(), True),\n","    StructField(\"link_flair_text\", StringType(), True),\n","    StructField(\"url\", StringType(), True),\n","    StructField(\"title\", StringType(), True),\n","    StructField(\"engagement_score\", DoubleType(), True),\n","    StructField(\"Total_Awards_Received\", IntegerType(), True),\n","    StructField(\"is_adult_content\", IntegerType(), True),\n","    StructField(\"is_spoiler\", IntegerType(), True),\n","    StructField(\"is_stickied\", IntegerType(), True),\n","    StructField(\"award_rate\", DoubleType(), True),\n","    StructField(\"score_upvote_ratio\", DoubleType(), True),\n","    StructField(\"title_length\", IntegerType(), True),\n","    StructField(\"post_age_days\", IntegerType(), True),\n","    StructField(\"engagement_ratio\", DoubleType(), True),\n","    StructField(\"has_awards\", IntegerType(), True),\n","    StructField(\"has_crossposts\", IntegerType(), True),\n","    StructField(\"Gilded_Count\", IntegerType(), True),\n","    StructField(\"Number_of_Crossposts\", IntegerType(), True),\n","    StructField(\"score\", IntegerType(), True),\n","    StructField(\"upvote_ratio\", FloatType(), True)\n","])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"dbdf5672-6fd7-410b-b31f-0e00edc71b66","normalized_state":"finished","queued_time":"2025-05-12T22:47:54.504049Z","session_start_time":null,"execution_start_time":"2025-05-12T22:48:02.8150344Z","execution_finish_time":"2025-05-12T22:48:03.1886523Z","parent_msg_id":"c945ab71-f9b2-4d18-ad56-413d94fda301"},"text/plain":"StatementMeta(, dbdf5672-6fd7-410b-b31f-0e00edc71b66, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c39f2ac6-c82c-495a-8730-2d3da6db6795"},{"cell_type":"markdown","source":["##### 3. Helper Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c2ec15a-c72d-4d74-8999-d38da08fe694"},{"cell_type":"code","source":["def get_max_id(database_name, table_name, pk_column):\n","    if spark.catalog.tableExists(f\"{database_name}.{table_name}\"):\n","        max_id = (\n","            spark.sql(f\"SELECT MAX({pk_column}) AS max_id FROM {database_name}.{table_name}\")\n","            .collect()[0][\"max_id\"]\n","        )\n","        return max_id if max_id is not None else 0\n","    return 0\n","\n","def process_table(database_name, table_name, pk_column, sk_column, sdf):\n","    \n","    sk_columns_mapping = {\n","        \"dim_author\": [\"pk_author_id\", \"author_name\"],\n","        \"dim_date\": [\"year\", \"month\", \"day\", \"weekday\"],\n","        \"dim_time\": [\"hour\"],\n","        \"dim_post\": [\"url\"],\n","        \"dim_engagement_attributes\": [\"is_adult_content\", \"is_spoiler\", \"is_stickied\", \"has_awards\", \"has_crossposts\"]\n","    }\n","\n","    pk_columns_mapping = {\n","        \"dim_author\": [\"author_name\"],\n","        \"dim_date\": [\"year\"],\n","        \"dim_time\": [\"hour\"],\n","        \"dim_post\": [\"url\"],\n","        \"dim_engagement_attributes\": [\"is_adult_content\"]\n","    }\n","\n","    pk_columns = pk_columns_mapping.get(table_name, [\"some_column\"])\n","    sk_columns = sk_columns_mapping.get(table_name, [c for c in sdf.columns if c != pk_column])\n","\n","    max_id = get_max_id(database_name, table_name, pk_column)\n","    window_spec = Window.orderBy(*pk_columns)\n","\n","    sdf = sdf.withColumn(pk_column, (row_number().over(window_spec) + max_id).cast(\"bigint\"))\n","\n","    sdf = sdf.withColumn(\n","        sk_column,\n","        crc32(md5(concat_ws(\"|\", *[coalesce(col(c).cast(\"string\"), lit(\"unknown\")) for c in sk_columns])))\n","    )\n","\n","    return sdf\n","\n","def add_date_time_columns(sdf):\n","    return sdf.withColumn(\"created_at\", from_unixtime(col(\"created_utc\"))) \\\n","              .withColumn(\"year\", F.year(\"created_at\")) \\\n","              .withColumn(\"month\", F.month(\"created_at\")) \\\n","              .withColumn(\"day\", F.dayofmonth(\"created_at\")) \\\n","              .withColumn(\"hour\", F.hour(\"created_at\"))\n","\n","def fill_missing_values(sdf):\n","    replacements = {\n","        \"author_name\": \"no_name\",\n","        \"num_comments\": 0,\n","        \"link_flair_text\": \"No Link Flair\",\n","        \"title\": \"No title for this submission\",\n","        \"over_18\": False,\n","        \"spoiler\": False,\n","        \"stickied\": False,\n","        \"Total_Awards_Received\": 0,\n","        \"Number_of_Crossposts\": 0\n","    }\n","    return sdf.fillna(replacements)\n","\n","def add_derived_columns(sdf):\n","    return sdf.withColumn(\"engagement_score\", col(\"score\") + col(\"num_comments\") * 0.5) \\\n","              .withColumn(\"is_adult_content\", when(col(\"over_18\") == True, 1).otherwise(0)) \\\n","              .withColumn(\"is_spoiler\", when(col(\"spoiler\") == True, 1).otherwise(0)) \\\n","              .withColumn(\"is_stickied\", when(col(\"stickied\") == True, 1).otherwise(0)) \\\n","              .withColumn(\"award_rate\", when(col(\"num_comments\") > 0, col(\"Total_Awards_Received\") / col(\"num_comments\")).otherwise(0.00)) \\\n","              .withColumn(\"score_upvote_ratio\", when(col(\"upvote_ratio\") > 0, col(\"score\") / col(\"upvote_ratio\")).otherwise(0.00)) \\\n","              .withColumn(\"title_length\", F.length(col(\"title\"))) \\\n","              .withColumn(\"post_age_days\", datediff(current_date(), F.to_date(from_unixtime(col(\"created_utc\"))))) \\\n","              .withColumn(\"engagement_ratio\", when(col(\"Total_Awards_Received\") > 0, col(\"num_comments\") / col(\"Total_Awards_Received\")).otherwise(0.00)) \\\n","              .withColumn(\"has_awards\", when(coalesce(col(\"Total_Awards_Received\"), lit(0)) > 0, 1).otherwise(0)) \\\n","              .withColumn(\"has_crossposts\", when(coalesce(col(\"Number_of_Crossposts\"), lit(0)) > 0, 1).otherwise(0)) \\\n","              .withColumn(\"weekday\", F.date_format(col(\"created_at\"), \"E\"))\n","\n","def save_to_metastore(spark_df, database_name, table_name, partition_columns=None):\n","    \"\"\"\n","    Saves Spark DataFrame to Delta table with merge functionality\n","    \n","    Args:\n","        sdf: Spark DataFrame to save\n","        database_name: Target database name\n","        table_name: Target table name\n","        partition_columns: List of columns to partition by (optional)\n","    \"\"\"\n","    # Validate input DataFrame\n","    if not isinstance(spark_df, DataFrame):\n","        raise ValueError(\"Input must be a Spark DataFrame\")\n","    \n","    # Create database if not exists\n","    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n","    full_table_name = f\"{database_name}.{table_name}\"\n","\n","    # Validate partition columns\n","    if partition_columns:\n","        missing_cols = [col for col in partition_columns if col not in spark_df.columns]\n","        if missing_cols:\n","            raise ValueError(f\"Partition columns not found in DataFrame: {missing_cols}\")\n","\n","\n","    # Write data to the table\n","    writer = spark_df.write.format(\"delta\") \\\n","        .mode(\"overwrite\") \\\n","        .option(\"mergeSchema\", \"true\")\n","\n","    if partition_columns:\n","\n","        writer = writer.partitionBy(*partition_columns)\n","\n","    writer.saveAsTable(full_table_name)\n","\n","    print(f\"Table {full_table_name} populated successfully.\")\n","\n","    # Refresh metadata and verify\n","    spark.catalog.refreshTable(full_table_name)\n","    print(f\"âœ… Successfully saved to '{full_table_name}'\")\n","    print(f\"ðŸ“Š Row count: {spark_df.count()}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"dbdf5672-6fd7-410b-b31f-0e00edc71b66","normalized_state":"finished","queued_time":"2025-05-12T22:59:27.2836819Z","session_start_time":null,"execution_start_time":"2025-05-12T22:59:27.2848491Z","execution_finish_time":"2025-05-12T22:59:27.6733175Z","parent_msg_id":"955b9a37-8b65-41ed-98fb-1dd165d1622f"},"text/plain":"StatementMeta(, dbdf5672-6fd7-410b-b31f-0e00edc71b66, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6d4520c-cbab-42dc-9955-39e168d49ff6"},{"cell_type":"markdown","source":["##### 4. Call Transformation Logic"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"915629bf-dc37-4d19-895e-26fd147c6b7d"},{"cell_type":"code","source":["# Load Bronze DataFrame\n","bronze_df = spark.read.format(\"delta\").table(\"bronze_layer.reddit_extracted_data\")\n","\n","# Apply transformations\n","df = add_date_time_columns(bronze_df)\n","df = fill_missing_values(df)\n","df = add_derived_columns(df)\n","\n","# Process dimensions\n","database = \"gold_dimensional_modeling\"\n","tables = [\n","    {\"table_name\": \"dim_author\", \"pk_column\": \"pk_author_id\", \"sk_column\": \"sk_author_id\"},\n","    {\"table_name\": \"dim_date\", \"pk_column\": \"pk_date_id\", \"sk_column\": \"sk_date_id\"},\n","    {\"table_name\": \"dim_time\", \"pk_column\": \"pk_time_id\", \"sk_column\": \"sk_time_id\"},\n","    {\"table_name\": \"dim_post\", \"pk_column\": \"pk_post_id\", \"sk_column\": \"sk_post_id\"},\n","    {\"table_name\": \"dim_engagement_attributes\", \"pk_column\": \"pk_engagement_id\", \"sk_column\": \"sk_engagement_id\"}\n","]\n","\n","for t in tables:\n","    df = process_table(database, t[\"table_name\"], t[\"pk_column\"], t[\"sk_column\"], df)\n","\n","# Select final columns\n","final_columns = [field.name for field in silver_layer_schema.fields]\n","df = df.select(final_columns)\n","df = spark.createDataFrame(df.rdd, schema=silver_layer_schema)\n","\n","# Save to Gold Layer\n","save_to_metastore(df, \"Silver_Layer\", \"Transformed_Data\", partition_columns=[\"year\", \"month\"])\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"dbdf5672-6fd7-410b-b31f-0e00edc71b66","normalized_state":"finished","queued_time":"2025-05-12T22:59:27.2970111Z","session_start_time":null,"execution_start_time":"2025-05-12T22:59:27.6755045Z","execution_finish_time":"2025-05-12T23:00:21.3320131Z","parent_msg_id":"5eb38d45-7543-42af-8852-f503b8d1daaf"},"text/plain":"StatementMeta(, dbdf5672-6fd7-410b-b31f-0e00edc71b66, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table Silver_Layer.Transformed_Data populated successfully.\nâœ… Successfully saved to 'Silver_Layer.Transformed_Data'\nðŸ“Š Row count: 38389\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad4ff63a-3fec-44d7-bd5c-3fc6268f4f23"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"05029953-93c2-40a0-b222-5656dc677253"}],"default_lakehouse":"05029953-93c2-40a0-b222-5656dc677253","default_lakehouse_name":"APIDataLakehouse","default_lakehouse_workspace_id":"e466565d-d48f-47c5-a36c-129e9706433f"}}},"nbformat":4,"nbformat_minor":5}